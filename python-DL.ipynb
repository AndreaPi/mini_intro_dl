{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A mini-intro to Deep Learning (focused on Computer Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: \n",
    "  - [David Yanofski](https://qz.com/author/davidyanofskyquartz/) for the ImageNet History image\n",
    "  - [Alex Bronstein and Avi Mendelson](https://vistalab-technion.github.io/cs236605/info/) for the blackboard-styled images on CNNs\n",
    "  - [Simone Scardapane](http://ispac.diet.uniroma1.it/scardapane/teaching/tensorflow-for-ict-applications/) for the Inception v3 and ResNet images\n",
    "  - [fast.ai](https://github.com/fastai/imagenette) for the Imagenette dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI (actually, Deep Learning) is around us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AI_around_us](images/AI_around_us.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# display few-shots adversarial learning for talking heads\n",
    "youtube_video = YouTubeVideo('p1b5aiTrGzY')\n",
    "display(youtube_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PS: I'm always looking for more impressive Deep Learning videos, thus if you know of something more amazing, do let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BIG_data](images/big_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI in Oil & Gas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ai_oil_gas](images/ai_oil_gas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: what is AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  At least 8 different definitions, according to the reference textbook\n",
    "- We define AI as the study of computer agents that interact with their environment, adapt to change, pursue goals and take actions to achieve the best expected outcome\n",
    "- AI is not new: its conventional birth date is the Darmouth conference (1956)\n",
    "- Big enthusiam in 1950-1970, but excessive optimism and failure to meet expectations led to AI winter in 1974 \n",
    "- In the â€˜80, success of expert systems (logic inference, symbolic AI) followed by another AI winter (today the biggest expert system, OpenCyc, doesn't have many applications, but less ambitious expert systems are used everyday)\n",
    "- We skip through the rest of AI history\n",
    "- The field of AI which is most successful today is **Machine Learning**, and in particular its subfield, **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hierarchy_ml](images/hierarchy_ml.png)  ![old_ml_def](images/old_ml_def.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative definition of Machine Learning\n",
    "- Machine Learning is informally defined as an algorithm which, given *data* and *labels* (for the simplest case of *supervised learning*) learns to associate data to labels, without being given an explicit association\n",
    "- There is however an alternative definition which better highlights the concept that Machine Learning is basically *efficient searching in the space of algorithms* \n",
    "- An algorithm ( is a sequence of steps (the instructions) each of which is simple and well defined, and that stops after a finite number of steps. \n",
    "- In many real-world problems, it is much easier to identify **desirable behavior** of an algorithm than to explicitly define the steps. Example: identify the presence and the breed of a dog in a picture\n",
    "- An algorithm is a **map** between input space and output space\n",
    "- An algorithm is a **point** in the space of such maps \n",
    "- Search the space of algorithms (informally, computer programs) until desired behavior is obtained\n",
    "- Surprisingly, it works!\n",
    "- Learning from examples instead of explicit modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet\n",
    "- The event which led to contemporary success, global interest and hype about AI was the 2012 edition of the ILSVRC competition, also known as ImageNet competition\n",
    "- Classify correctly 1.2 millions of images, 1000 classes\n",
    "![dogs](images/dogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ImageNet moment\n",
    "- In 2010 (ILSVRC first year), every competing team got at least 25% images wrong\n",
    "- In 2012, using Deep Learning, team of AI legend Geoffrey Hinton reduced the error rate by nearly 50%\n",
    "- This was a quantum leap, with respect to usual advance rate in AI\n",
    "- Deep Learning has been used by ILSVRC winning teams since 2012, and itâ€™s now being used in all the fields cited at the beginning of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenet_history](images/imagenet_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using huge ($O(10^6) Ã· O(10^9)$ of parameters) neural networks (NN) as the trainable model in Machine Learning\n",
    "- Backpropagation-trained NN have been around since at least 1986, however the Â«oldÂ» NN were much smaller (~ 25 parameters)\n",
    "- Only later, hardware (GPU), large amount of data and algoritmic innovations (SGD, dropout, BatchNorm, residual connections, Fixup initialization, etc.) made possible to train Deep neural networks\n",
    "- **The huge amount of money being spent by FAANG on Deep Learning is making the field progress as never before (BigGAN, VQ-VAE, XLNet, GPT-2, RoBERTa, MEGATRON, etc.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ok, how do we do classification on images in practice?\n",
    "- An image of width $W$, height $H$ and with $C$ channels can be represented as a $W\\times H\\times C$ tensor\n",
    "- Images are **BIG**: a small 256x256 RGB image is a tensor of $3\\times256\\times256\\approx 2 \\cdot 10^5$ components, the R, G and B intensities for each pixel. \n",
    "  - a classical MLP (fully connected, feed-forward neural network) with 512 neurons in the first hidden layer, would burn $(3\\times256\\times256+1)\\times512\\approx \\cdot 10^8$ only in the **first hidden layer**, and we want to go deep (many layers!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](images/mlp3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Learning is **HUNGRY**: $N=1.2\\cdot10^6$ for ILSVRC! (but does this mean that we cannot use Deep Learning on small datasets? **NO** (transfer learning))\n",
    "- It turns out we can solve the first problem, and simplify the second one, using the same ideas\n",
    "- The right layer for image representation should be **translation-equivariant**: if I shift the image in any direction, its representation in feature space should just shift in the same way\n",
    "\n",
    "$$\\tau_p \\mathcal{H}= \\mathcal{H} \\tau_p \\ \\forall p \\in \\mathbb{â„¤}^d$$\n",
    "\n",
    "- This makes it easier to build classifiers which are **translation-invariant**\n",
    "- Also, the representation should be sensitive to **local** features: the pixels identifying an eye in a face are all adjacent\n",
    "- Q: how do we get **locality** and **translation-equivariance**? A: weight sharing, a.k.a **filters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![weight_sharing](images/weight_sharing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- obviously, weight sharing also reduces the number of parameters in our neural network, and it thus allows us to handle bigger datasets with the same computational power\n",
    "- Weight sharing/filters leads to the concept of the **convolutional layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![toeplitz](images/toeplitz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![conv2](images/conv_layer_1.png) ![conv2](images/conv_layer_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional layers are just one of the technical innovations which make modern CNNs so good at classifying images:\n",
    "  - batch normalization/spectral normalization\n",
    "  - max pooling\n",
    "  - global pooling \n",
    "  - strided convolutions\n",
    "  - atrous convolutions\n",
    "  - the skip/residual connection, which enhances the NN **topological expressivity**\n",
    "  - dropout (rapidly becoming obsolete)\n",
    "  - new initializations, training schedules, label modifications, data augmentation tricks, etc.\n",
    "- Listing them all would take way too much time\n",
    "- Let's just mention two of the most famous/robust CNN architectures in use today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception\n",
    "- from Google \n",
    "- various version, we show v1 (2014) but v3 is the most commonly used today, as well as hybridizations with ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception](images/inception.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "- a family of architectures from Microsoft Research, which introduced the residual connection\n",
    "- ResNet-50 (50 layers) is the most commonly used today, but ResNet-34 (picture) and ResNet-101 are also widely adopted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnet](images/resnet-34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough theory! Let's start coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three dominant Deep Learning frameworks/API today are [Tensorflow](https://www.tensorflow.org/), [Keras](https://keras.io/) and [PyTorch](https://pytorch.org/) (Tensorflow and Keras were recently unified with the release of Tensorflow 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![frameworks](images/deep_learning_frameworks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **Keras** for our demonstration because it's a very high level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Infererence\n",
    "Let's start simple: we'll just download a ResNet50 model, pretrained on the subsets of ImageNet used for ILSVRC, and try to classify this image:\n",
    "\n",
    "![elephant](images/elephant.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we proceed, which class would you attribute to this image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![question-mark](images/question-mark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import requests\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import tarfile\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "# uncomment to set numpy random seed\n",
    "# np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = 'images/elephant.jpg'\n",
    "\n",
    "# resize the image to the size commonly used with ResNet\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# preprocess the image (this corresponds to normalizing data in a classical Machine Learning model)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch), and return the three most probable classes\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you guess \"elephant\" or \"African elephant\"? In the first case, the CNN was smarter than you ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training\n",
    "Now you will learn **why** you need a GPU to perform any serious Deep Learning. Let's try to train ResNet50 on a **smaller, simpler version of ImageNet**. **WARNING**: this is going to download a 1.56 Gb file on your pc, thus if you're not comfortable with that, skip this session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "\n",
    "url = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette.tgz'\n",
    "a = urlparse(url)\n",
    "file_name = os.path.basename(a.path)\n",
    "if not os.path.exists(file_name):\n",
    "    # archived dataset is missing, download from S3\n",
    "    response = requests.get(url, stream=True, verify=False)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kbyte\n",
    "    t=tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        for data in response.iter_content(block_size):\n",
    "            t.update(len(data))\n",
    "            f.write(data)\n",
    "        t.close()\n",
    "    if total_size != 0 and t.n != total_size:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "\n",
    "# uncompress the tar.gz archive\n",
    "base_path = os.path.splitext(file_name)[0]\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    # we have the archive but not the uncompressed folder, so let's uncompress the archive\n",
    "    tar = tarfile.open(file_name, \"r:gz\")\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "bs = 64\n",
    "epochs = 180\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define image generators\n",
    "See here for the full documentation of the Keras image generator: https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths & logging\n",
    "train_path=os.path.join(base_path, 'train')\n",
    "val_path=os.path.join(base_path, 'val')\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"resnet50-generator-%Y%m%d-%H%M%S\")\n",
    "\n",
    "# create image generators\n",
    "train_generator = image.ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rotation_range=40,\n",
    "    shear_range=0.2,\n",
    "    channel_shift_range=0.2,\n",
    "    fill_mode='reflect',\n",
    "    horizontal_flip=True)\n",
    "\n",
    "val_generator = image.ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "tgen = train_generator.flow_from_directory(directory=train_path,\n",
    "                                           target_size=(img_rows, img_cols),\n",
    "                                           color_mode='rgb',\n",
    "                                           batch_size=bs,\n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=True)\n",
    "vgen = val_generator.flow_from_directory(val_path,\n",
    "                                         target_size=(img_rows, img_cols),\n",
    "                                         color_mode='rgb', \n",
    "                                         class_mode='categorical', \n",
    "                                         batch_size=bs, \n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "# define model (start from random weights, or it's too easy!!)\n",
    "model = ResNet50(weights=None, classes=nb_classes)\n",
    "\n",
    "# callbacks\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "snapshot=\"snapshot/ep:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(snapshot, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, tensorboard_callback]\n",
    "\n",
    "# compile model\n",
    "print('compiling ..')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "If done on a CPU, this will probably take weeks (consumer laptop, 2019). To train on a GPU, remember that you must have used the `requirements_gpu.txt` file before, instead than `requirements.txt` (and ofc have an enviroment with a GPU available!). To check that you have a GPU available and that Keras sees it, you can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm that Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "print('training ..')\n",
    "model.fit_generator(tgen,\n",
    "                   epochs=epochs,\n",
    "                   steps_per_epoch=tgen.n/bs,\n",
    "                   validation_steps=vgen.n/bs,\n",
    "                   validation_data=vgen,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction again, but this time on one of the 10 classes in Imagenette\n",
    "# we're choosing an image of class [chainsaw]\n",
    "class_path = os.path.join(base_path, 'val','n03000684')\n",
    "_, _, images = next(os.walk(class_path))\n",
    "idx = np.random.choice(len(images))\n",
    "\n",
    "img_path = os.path.join(class_path, images[idx])\n",
    "\n",
    "# resize the image to the size commonly used with ResNet\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# preprocess the image (this corresponds to normalizing data in a classical Machine Learning model)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
